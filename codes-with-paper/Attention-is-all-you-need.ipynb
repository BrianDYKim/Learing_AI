{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74e9f56-07bd-4118-a8a2-f2f58442d983",
   "metadata": {},
   "source": [
    "## Attention is all you need 실습\n",
    "- 본 코드는 기본적으로 Attention is all you need (NIPS 2017) 논문의 내용을 최대한 따릅니다.\n",
    "- 다만 일부 코드의 경우 Attention is all you need 내용을 따르지 않고 최신 아키텍처를 반영한 부분도 존재합니다. (가령, Positional Embedding 부분)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a6c4-ef01-4448-b4ea-1805a341a40b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>데이터 전처리 (PreProcessing)</b>\n",
    "\n",
    "**실행 커맨드**\n",
    "\n",
    "```sh\n",
    "conda install -n playground pytorch\n",
    "conda install -n huggingface\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a91a0-1782-4314-86d1-c64a91b0692c",
   "metadata": {},
   "source": [
    "- 허깅페이스 API를 이용해서 대표적인 영어-독어 데이터셋인 **Multi30k**를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "22e6ea19-30df-4eff-be62-807082fc33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face에서 데이터셋 로드\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c1e6f695-be19-4b56-a45e-e7df52574bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Two young, White males are outside near many bushes.', 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = dataset['train'], dataset['validation'], dataset['test']\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a6e79-312e-4d39-adfa-6af16a5fdaf9",
   "metadata": {},
   "source": [
    "- **Tokenizer** 및 **Vocab** 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4212c2f8-f817-467d-a38c-f78576ee3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6129280c-d864-42fa-87ad-c036550fa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level tokenizer 초기화\n",
    "unknown_token = \"<unk>\"\n",
    "\n",
    "# 영어 토크나이저 정의 (Word Level)\n",
    "en_tokenizer = Tokenizer(WordLevel(unk_token=unknown_token))\n",
    "en_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# 독일어 토크나이저 정의 (Word Level)\n",
    "de_tokenizer = Tokenizer(WordLevel(unk_token=unknown_token))\n",
    "de_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e647190d-0467-4f21-a78f-6c23be03f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 trainer 설정 (vocab 생성)\n",
    "pad_token, sos_token, eos_token = \"<pad>\", \"<sos>\", \"<eos>\"\n",
    "special_tokens = [unknown_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=special_tokens, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8a70fe9f-edee-4693-b62e-ae26f6daad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 학습 (문장 리스트 기반)\n",
    "train_en = train_dataset['en']\n",
    "train_de = train_dataset['de']\n",
    "\n",
    "en_tokenizer.train_from_iterator(train_en, trainer=trainer)\n",
    "de_tokenizer.train_from_iterator(train_de, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "587e362b-bfd4-4c85-bd14-fe342ad8eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EN] vocab size: 6203\n",
      "[DE] vocab size: 8060\n",
      "[EN] Sample EN vocab tokens: ['zooms', 'aged', 'pressure', 'football', 'sofa', 'washer', 'rally', 'hides', 'mood', 'i']\n",
      "[DE] Sample DE vocab tokens: ['Vase', 'Cowgirl', 'Gepäck', 'Karateanzügen', 'Sommer', 'Eiffelturm', 'karierten', 'Absperrung', 'gerade', 'Löwen']\n"
     ]
    }
   ],
   "source": [
    "# vocab 확인\n",
    "\n",
    "print(\"[EN] vocab size: {}\".format(en_tokenizer.get_vocab_size()))\n",
    "print(\"[DE] vocab size: {}\".format(de_tokenizer.get_vocab_size()))\n",
    "\n",
    "print(\"[EN] Sample EN vocab tokens: {}\".format(list(en_tokenizer.get_vocab().keys())[:10]))\n",
    "print(\"[DE] Sample DE vocab tokens: {}\".format(list(de_tokenizer.get_vocab().keys())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "98fafa2f-2e9f-4d3a-aaf5-c736c8a6085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EN] special_token: <unk>, index: 0\n",
      "[DE] special token: <unk>, index: 0\n",
      "[EN] special_token: <pad>, index: 1\n",
      "[DE] special token: <pad>, index: 1\n",
      "[EN] special_token: <sos>, index: 2\n",
      "[DE] special token: <sos>, index: 2\n",
      "[EN] special_token: <eos>, index: 3\n",
      "[DE] special token: <eos>, index: 3\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 인덱스 체크\n",
    "\n",
    "# <unk> : 0\n",
    "# <pad> : 1\n",
    "# <sos> : 2\n",
    "# <eos> : 3\n",
    "for special_token in special_tokens:\n",
    "    print(\"[EN] special_token: {}, index: {}\".format(special_token, en_tokenizer.get_vocab()[special_token]))\n",
    "    print(\"[DE] special token: {}, index: {}\".format(special_token, de_tokenizer.get_vocab()[special_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3e1e7-76a6-411b-8040-8e04561d837a",
   "metadata": {},
   "source": [
    "- 하이퍼 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ca913818-e12f-423f-a357-bf77a55d417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfiguration:\n",
    "    def __init__(self, \n",
    "                 max_len: int = 768, \n",
    "                 batch_size: int = 32, \n",
    "                 hidden_size: int = 512, \n",
    "                 ffn_size: int = 2048,\n",
    "                 num_heads: int = 8, \n",
    "                 num_layers: int = 6, \n",
    "                 dropout_pb: float = 0.1, \n",
    "                 src_vocab_size: int = 0, \n",
    "                 trg_vocab_size: int = 0\n",
    "                ):\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_size = ffn_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_pb = dropout_pb\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "\n",
    "model_config = ModelConfiguration(src_vocab_size=de_tokenizer.get_vocab_size(), trg_vocab_size=en_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0d18e-2cef-474e-a572-b8bdc658fa1a",
   "metadata": {},
   "source": [
    "- 데이터 전처리\n",
    "  - 데이터 패딩 등.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "86833fa3-645c-4840-ac1a-73ce44df1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 언어 사전에서 특수 토큰 (sos, eos) id\n",
    "en_sos_id, en_eos_id = en_tokenizer.token_to_id(sos_token), en_tokenizer.token_to_id(eos_token)\n",
    "de_sos_id, de_eos_id = de_tokenizer.token_to_id(sos_token), de_tokenizer.token_to_id(eos_token)\n",
    "de_pad_id, en_pad_id = de_tokenizer.token_to_id(pad_token), en_tokenizer.token_to_id(pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "59eefa03-45fa-4c3c-8f60-57dfdeae8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: {\"en\" : \"example_en\", \"de\" : \"example_de\"}\n",
    "def preprocess(dataset: dict) -> dict:\n",
    "    max_len = model_config.max_len\n",
    "    batch_size = model_config.batch_size\n",
    "    \n",
    "    # 토큰 id로 변환\n",
    "    src_input_ids = de_tokenizer.encode(dataset['de']).ids\n",
    "    trg_input_ids = en_tokenizer.encode(dataset['en']).ids\n",
    "\n",
    "    # decoder 출력 부분에 special tokens 추가\n",
    "    # I am a student 라는 문장이 있다면, 출력은 <sos> -> I, I -> am, ... 순으로 예측을 하기 때문\n",
    "    decoder_input = [en_sos_id] + trg_input_ids\n",
    "    labels = trg_input_ids + [en_eos_id]\n",
    "\n",
    "    # padding\n",
    "    encoder_input = src_input_ids[:max_len] + [de_pad_id] * max(0, max_len - len(src_input_ids))\n",
    "    decoder_input = decoder_input[:max_len] + [en_pad_id] * max(0, max_len - len(decoder_input))\n",
    "    labels = labels[:max_len] + [en_pad_id] * max(0, max_len - len(labels))\n",
    "\n",
    "    # Attention mask (1 if real token else 0)\n",
    "    encoder_attention_mask = [1 if token != de_pad_id else 0 for token in encoder_input]\n",
    "    decoder_attention_mask = [1 if token != en_pad_id else 0 for token in decoder_input]\n",
    "\n",
    "    return {\n",
    "        \"encoder_input_ids\" : encoder_input,\n",
    "        \"encoder_attention_mask\" : encoder_attention_mask,\n",
    "        \"decoder_input_ids\" : decoder_input,\n",
    "        \"decoder_attention_mask\" : decoder_attention_mask,\n",
    "        \"labels\" : labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ea991ee1-f5ed-4119-aac8-b5f9e19c7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 적용\n",
    "train_dataset = train_dataset.map(preprocess, remove_columns=['en', 'de'])\n",
    "validation_dataset = validation_dataset.map(preprocess, remove_columns=['en', 'de'])\n",
    "test_dataset = test_dataset.map(preprocess, remove_columns=['en', 'de'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4ebd145c-1dc1-4e21-954e-8361fe9e2998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoder_input_ids': [14, 5654, 10, 810, 28, 8, 19, 4270, 276, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'encoder_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'decoder_input_ids': [2, 6, 1886, 635, 13, 721, 111, 97, 7, 5913, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'decoder_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [6, 1886, 635, 13, 721, 111, 97, 7, 5913, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 전처리된 데이터셋 검증\n",
    "print(train_dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1f9b00c3-2c55-47ad-b0cc-1f1da6ce9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# collate function\n",
    "def collate_function(batch):\n",
    "    return {\n",
    "        key: torch.tensor([data[key] for data in batch], dtype=torch.long) for key in batch[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "23d6e568-b192-4c9e-b63c-beae143c907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader 생성\n",
    "batch_size = model_config.batch_size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_function)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_function)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "21a9378a-599f-4b4b-ace0-2503334b946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_ids: shape=torch.Size([32, 768])\n",
      "encoder_attention_mask: shape=torch.Size([32, 768])\n",
      "decoder_input_ids: shape=torch.Size([32, 768])\n",
      "decoder_attention_mask: shape=torch.Size([32, 768])\n",
      "labels: shape=torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "# 배치 샘플 확인\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "for key, value in batch.items():\n",
    "    print(\"{}: shape={}\".format(key, value.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085f72b-c7b5-410d-88ad-f55010f00d50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>토큰 임베딩</b>\n",
    "- Attention is all you need 에서 소개된 토큰 임베딩은 크게 token 자체 임베딩, 위치 임베딩 둘을 합쳐서 구현합니다.\n",
    "  - output = token_embedding(input_ids) + positional_embedding(input_ids)\n",
    "  - positional embedding의 경우, Transformer 이전의 Seq2Seq 아키텍처 까지는 Residual block 들이 위치를 보장해주는 역할을 수행하였으나, Transformer 아키텍처 부터는 위치를 보장해주는 장치가 없어지다보니, 위치를 보장해주는 임베딩을 처리해야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e0f8f8ad-5d73-4491-b553-3ca76dbff99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 device 정의\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c6125375-a9af-4683-8ace-5fbda39c57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_len: int, dropout_pb: float):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.positional_embedding = nn.Embedding(max_len, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(dropout_pb)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # 입력 시퀀스에 대한 positional ids 생성\n",
    "        sequence_len = input_ids.size(1)\n",
    "        positional_ids = torch.arange(sequence_len, device=device).unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        # 임베딩\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        positional_embeddings = self.positional_embedding(positional_ids)\n",
    "\n",
    "        # Add/Norm + Dropout\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "15a57498-e8f3-4413-a2cb-18c48c3b81d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 768])\n",
      "Embedding shape: torch.Size([32, 768, 512])\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 검증\n",
    "embedding_layer = Embeddings(\n",
    "    vocab_size = model_config.src_vocab_size,\n",
    "    hidden_size= model_config.hidden_size,\n",
    "    max_len = model_config.max_len,\n",
    "    dropout_pb = model_config.dropout_pb\n",
    ").to(device)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "input_ids = batch['encoder_input_ids'].to(device)\n",
    "\n",
    "embeddings = embedding_layer(input_ids)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Input shape: {}\".format(input_ids.shape))\n",
    "print(\"Embedding shape: {}\".format(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484a47d6-25b3-4692-ba03-220018b0b69f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>Multi Head Attention 구현</b>\n",
    "\n",
    "- Transformer 아키텍처의 핵심인 멀티 헤드 어텐션을 구현합니다.\n",
    "  - scaled-dot-product attention 구현\n",
    "  - attention head 구현\n",
    "  - Attention Head를 조합하여 Multi Head Attention 구현\n",
    "\n",
    "#### Attention 공식\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right)V\n",
    "$$\n",
    "\n",
    "#### Masked Attention 공식\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} + \\text{mask} \\right)V\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91f18abd-de95-4dd0-9b65-38664602a188",
   "metadata": {},
   "source": [
    "- **점곱 어텐션** 구현\n",
    "<img src=\"./resources/attention-is-all-you-need/scaled-dot-product-attention.png\" alt=\"점곱 어텐션\" width=\"400\" height=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d2351d18-423c-47c4-b6c4-92bec61c9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Scaled dot product attention 구현\n",
    "def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "    # hidden size\n",
    "    dim_k = query.size(-1)\n",
    "\n",
    "    # Attention score 계산\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / (dim_k ** 0.5)\n",
    "\n",
    "    # mask가 존재하면 -1e9를 더한다\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # softmax로 attention wights\n",
    "    attention_weights = F.softmax(scores, dim = -1)\n",
    "\n",
    "    # attention * value\n",
    "    output = torch.bmm(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b23ca38e-51e1-499d-91f2-d2e6724aed07",
   "metadata": {},
   "source": [
    "- **Attention Head** 구현\n",
    "  - 아래 그림 중에서, 헤드 부분을 구현합니다.\n",
    " \n",
    "<img src=\"./resources/attention-is-all-you-need/multi-head-attention.png\" alt=\"멀티헤드 어텐션\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dae95d9b-3497-4cdb-ab63-bbda1f92383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, head_dim: int):\n",
    "        super().__init__()\n",
    "        self.query_projection = nn.Linear(hidden_dim, head_dim)\n",
    "        self.key_projection = nn.Linear(hidden_dim, head_dim)\n",
    "        self.value_projection = nn.Linear(hidden_dim, head_dim)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None):\n",
    "        Q = self.query_projection(query)\n",
    "        K = self.key_projection(key)\n",
    "        V = self.value_projection(value)\n",
    "\n",
    "        # mask까지 전달\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7afcd07-fc03-423f-98c2-efaaa1b3f3e1",
   "metadata": {},
   "source": [
    "- **Multi Head Attention** 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "673afd7c-fadb-4289-835f-340b8e1beabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.head_list = nn.ModuleList([AttentionHead(hidden_dim, head_dim) for _ in range(num_heads)])\n",
    "        self.output_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        concat_attention = torch.concat([head(query, key, value, mask) for head in self.head_list], dim = -1)\n",
    "        linear_output = self.output_linear(concat_attention)\n",
    "\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f63496-3bb1-420a-b9ef-b6b7072852f6",
   "metadata": {},
   "source": [
    "- 멀티 헤드 어텐션 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "99d31360-274e-418e-a740-2a02177e7860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768, 512])\n"
     ]
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(hidden_dim = model_config.hidden_size, num_heads = model_config.num_heads).to(device)\n",
    "\n",
    "# 이전 테스트에서 임베딩된 값을 입력으로 제공\n",
    "attn_output = multi_head_attention(embeddings, embeddings, embeddings)\n",
    "\n",
    "print(attn_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a7d8d-c3fc-4f57-97a5-9929d5dcfe2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>Positionwise Feed Forward 계층 설계</b>\n",
    "\n",
    "- 트랜스포머 인코더와 디코더 계층 사이에 존재하는 완전 연결 신경망 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d84484e3-c016-4933-83cf-e3355663d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, ffn_dim: int, dropout_pb: float):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(hidden_dim, ffn_dim)\n",
    "        self.linear2 = nn.Linear(ffn_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_pb)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821cf85-8480-4058-9c78-7759c9fbcaa8",
   "metadata": {},
   "source": [
    "- 피드 포워드 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6846f759-6e6b-4b2c-a1fd-58b80a5053e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768, 512])\n"
     ]
    }
   ],
   "source": [
    "feed_forward_layer = PositionwiseFeedForward(model_config.hidden_size, model_config.ffn_size, model_config.dropout_pb).to(device)\n",
    "\n",
    "ff_outputs = feed_forward_layer(attn_output)\n",
    "\n",
    "print(ff_outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885ceee-e131-4e09-973f-b424b6564ccd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>Mask 생성 함수 구현</b>\n",
    "\n",
    "- Transformer의 경우 masking 작업이 필요한데, 크게 아래의 두가지 마스크가 필요하다\n",
    "  - **Masked Self Attention** 에서 사용되는 mask : 미래 단어를 look ahead를 하여 부정 예측을 하는 현상을 막기 위함\n",
    "  - **Encoder Decoder Attention** 에서 사용되는 mask : encoder에서 들어오는 <pad> 토큰을 예측에 반영하지 않도록 막기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "61eb7520-b8b0-4446-b8c1-3b6449d77062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look ahead mask 생성 함수\n",
    "def create_look_ahead_mask(seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "    # 1로 채워진 하삼각행렬 생성\n",
    "    # (seq_len, seq_len)\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len), device = device).bool()\n",
    "    return mask\n",
    "\n",
    "# <pad> 를 무시하도록 padding mask 생성 함수\n",
    "def create_padding_mask(input_ids: torch.Tensor, pad_token_id: int) -> torch.Tensor:\n",
    "    # 브로드캐스팅을 위한 차원 확장\n",
    "    # input_ids: (batch_size, seq_len)\n",
    "    # → shape: (batch_size, 1, 1, seq_len) (dim = -1 부분이 mask 벡터가 되도록)\n",
    "    return (input_ids != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_mask(\n",
    "    src_input_ids: torch.Tensor,\n",
    "    trg_input_ids: torch.Tensor,\n",
    "    src_pad_id: int,\n",
    "    trg_pad_id: int,\n",
    "    device: torch.device\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Encoder padding mask\n",
    "    # (batch_size, 1, 1, src_len)\n",
    "    memory_mask = create_padding_mask(src_input_ids, src_pad_id)\n",
    "\n",
    "    # Decoder padding mask (batch_size, 1, 1, trg_len)\n",
    "    trg_padding_mask = create_padding_mask(trg_input_ids, trg_pad_id)\n",
    "\n",
    "    # Decoder Look-ahead mask (1, trg_len, trg_len)\n",
    "    look_ahead_mask = create_look_ahead_mask(trg_input_ids.size(-1), device).unsqueeze(0)\n",
    "\n",
    "    # Decoder의 경우 padding mask와 look ahead를 동시에 만족해야함\n",
    "    trg_mask = look_ahead_mask & trg_padding_mask.squeeze(1).squeeze(1).unsqueeze(1)\n",
    "\n",
    "    return trg_mask, memory_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2990ce73-d85d-4694-91bf-56caa5d6e9ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>Transformer Encoder 설계</b>\n",
    "\n",
    "- Encoder Stack 정의\n",
    "- Encoder Stack을 여러개 쌓아서 Transformer Encoder 생성\n",
    "\n",
    "**트랜스포머 도식**\n",
    "<img src=\"./resources/attention-is-all-you-need/transformer-architecture.webp\" alt=\"트랜스포머 아키텍처\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833b238-f7ad-4980-ba09-c713ff02ec29",
   "metadata": {},
   "source": [
    "- **트랜스포머 인코더 스택** 정의\n",
    "  - 본 논문에서는 사후 Normalization 기반으로 설명하고 있으나, 성능상으로는 **사전 Normalization** 이 더 우수하다고 알려져있기에, 본 코드에서는 사전 Normalization을 기반으로 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "db59eb31-b2b3-47ef-84ad-dec1e0161c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, ffn_dim: int, num_heads: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.feed_forward_layer = PositionwiseFeedForward(hidden_dim, ffn_dim, dropout_prob)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n",
    "        # Pre Normalization\n",
    "        x_norm1 = self.norm1(x)\n",
    "\n",
    "        # Self Attention\n",
    "        attention_output = self.self_attention(x_norm1, x_norm1, x_norm1, mask)\n",
    "\n",
    "        # Dropout + Highway Adding\n",
    "        x = x + self.dropout1(attention_output)\n",
    "\n",
    "        # Pre Normalization\n",
    "        x_norm2 = self.norm2(x)\n",
    "\n",
    "        # Feed Forward\n",
    "        ffn_output = self.feed_forward_layer(x_norm2)\n",
    "\n",
    "        # Dropout + Highway Adding\n",
    "        x = x + self.dropout2(ffn_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd87949-9a5c-4d25-a5a7-ddd0d8a0df9b",
   "metadata": {},
   "source": [
    "- **트랜스포머 인코더** 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6071a334-d990-48c9-85b9-2ff21d18f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, hidden_dim: int, ffn_dim: int, num_heads: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(hidden_dim, ffn_dim, num_heads, dropout_prob) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e46aac-ec1c-4a05-92e0-50a23bf5f909",
   "metadata": {},
   "source": [
    "- 트랜스포머 인코더 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "362e9a97-e5bc-49bc-899a-c32bbd023933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768, 512])\n"
     ]
    }
   ],
   "source": [
    "example_encoder = Encoder(model_config.num_layers, model_config.hidden_size, model_config.ffn_size, model_config.num_heads, model_config.dropout_pb).to(device)\n",
    "encoder_output = example_encoder(embeddings)\n",
    "\n",
    "print(encoder_output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da9fb59-f97e-4eac-a4e5-4da168a21e5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>Transformer Decoder 설계</b>\n",
    "\n",
    "- Masked Multi Head Attention 정의\n",
    "- Decoder Stack 정의\n",
    "- Decoder Stack을 여러개 쌓아서 Transformer Decoder 설계\n",
    "- 출력 계층 정의\n",
    "\n",
    "**트랜스포머 도식**\n",
    "<img src=\"./resources/attention-is-all-you-need/transformer-architecture.webp\" alt=\"트랜스포머 아키텍처\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9c1de-d538-4d98-9c93-0806129116bf",
   "metadata": {},
   "source": [
    "- **Decoder Layer** 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "13b4346b-e2cc-4fee-87f2-ef3bf67dcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, ffn_dim: int, num_heads: int, dropout_pb: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(hidden_dim, num_heads)\n",
    "        self.feed_forward_layer = PositionwiseFeedForward(hidden_dim, ffn_dim, dropout_pb)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout_pb)\n",
    "        self.dropout2 = nn.Dropout(dropout_pb)\n",
    "        self.dropout3 = nn.Dropout(dropout_pb)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, trg_mask: torch.Tensor = None, memory_mask: torch.Tensor = None):\n",
    "        # Masked Self Attention\n",
    "        # 디코더가 예측 중에 다음 단어를 참고한 추론을 수행하지 못하도록 trg_mask를 적용\n",
    "        x_norm = self.norm1(x)\n",
    "        self_attention_output = self.self_attention(x_norm, x_norm, x_norm, trg_mask)\n",
    "        x = x + self.dropout1(self_attention_output)\n",
    "\n",
    "        # Encoder-Decoder Attention\n",
    "        # encoder-decoder attention 수행 과정 중에 <pad> 부분을 추론에 이용하지 못하도록 memory_mask 적용\n",
    "        x_norm = self.norm2(x)\n",
    "        encoder_decoder_attention_output = self.encoder_decoder_attention(x_norm, encoder_output, encoder_output, memory_mask)\n",
    "        x = x + self.dropout2(encoder_decoder_attention_output)\n",
    "\n",
    "        # Feed-Forward\n",
    "        x_norm = self.norm3(x)\n",
    "        feed_forward_layer_output = self.feed_forward_layer(x_norm)\n",
    "        x = x + self.dropout3(feed_forward_layer_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2463073-df7e-408e-aef8-54b58e468efc",
   "metadata": {},
   "source": [
    "- **Transformer Decoder** 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "a8758743-0881-4fa7-8a05-c77e6156c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, hidden_dim: int, ffn_dim: int, num_heads: int, dropout_pb: float):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(hidden_dim, ffn_dim, num_heads, ffn_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, trg_mask: torch.Tensor = None, memory_mask: torch.Tensor = None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, trg_mask, memory_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09924e6-fefd-4f4f-87aa-e036961e1439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### <b>Transformer Encoder, Decoder 통합하여 모델 완성</b>\n",
    "\n",
    "<img src=\"./resources/attention-is-all-you-need/transformer-architecture.webp\" alt=\"트랜스포머 아키텍처\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "01ede6a4-e787-4420-92cc-4e73550cbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfiguration):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding Layers\n",
    "        self.src_embedding = Embeddings(config.src_vocab_size, config.hidden_size, config.max_len, config.dropout_pb)\n",
    "        self.trg_embedding = Embeddings(config.trg_vocab_size, config.hidden_size, config.max_len, config.dropout_pb)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(config.num_layers, config.hidden_size, config.ffn_size, config.num_heads, config.dropout_pb)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(config.num_layers, config.hidden_size, config.ffn_size, config.num_heads, config.dropout_pb)\n",
    "\n",
    "        # Final Linear Projection\n",
    "        self.output_linear = nn.Linear(config.hidden_size, config.trg_vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        src_input_ids: torch.Tensor,\n",
    "        trg_input_ids: torch.Tensor,\n",
    "        trg_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Embedding\n",
    "        encoder_input = self.src_embedding(src_input_ids)\n",
    "        decoder_input = self.trg_embedding(trg_input_ids)\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output = self.encoder(encoder_input)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, trg_mask, memory_mask)\n",
    "\n",
    "        # Final Linear\n",
    "        logits = self.output_linear(decoder_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd01e98-39ca-4f5c-af97-84a69d034733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c62ac-0c00-451c-8fb3-5cdc04a49eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlps-2017",
   "language": "python",
   "name": "nlps-2017"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
