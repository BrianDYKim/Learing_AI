{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d74e9f56-07bd-4118-a8a2-f2f58442d983",
   "metadata": {},
   "source": [
    "## Attention is all you need 실습\n",
    "- 본 코드는 기본적으로 Attention is all you need (NIPS 2017) 논문의 내용을 최대한 따릅니다.\n",
    "- 다만 일부 코드의 경우 Attention is all you need 내용을 따르지 않고 최신 아키텍처를 반영한 부분도 존재합니다. (가령, Positional Embedding 부분)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d5288-f126-411e-bbbc-f72637b9a903",
   "metadata": {},
   "source": [
    "#### <b>하이퍼 파라미터 정의</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ca913818-e12f-423f-a357-bf77a55d417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"max_len\" : 128, # 시퀀스 최대 길이\n",
    "    \"batch_size\" : 32 # 배치 크기\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a6c4-ef01-4448-b4ea-1805a341a40b",
   "metadata": {},
   "source": [
    "#### <b>데이터 전처리 (PreProcessing)</b>\n",
    "\n",
    "**실행 커맨드**\n",
    "```sh\n",
    "conda install -n playground pytorch\n",
    "conda install -n huggingface\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a91a0-1782-4314-86d1-c64a91b0692c",
   "metadata": {},
   "source": [
    "- 허깅페이스 API를 이용해서 대표적인 영어-독어 데이터셋인 **Multi30k**를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "22e6ea19-30df-4eff-be62-807082fc33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face에서 데이터셋 로드\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c1e6f695-be19-4b56-a45e-e7df52574bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Two young, White males are outside near many bushes.', 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset, test_dataset = dataset['train'], dataset['validation'], dataset['test']\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a6e79-312e-4d39-adfa-6af16a5fdaf9",
   "metadata": {},
   "source": [
    "- **Tokenizer** 및 **Vocab** 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4212c2f8-f817-467d-a38c-f78576ee3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6129280c-d864-42fa-87ad-c036550fa67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level tokenizer 초기화\n",
    "unknown_token = \"<unk>\"\n",
    "\n",
    "# 영어 토크나이저 정의 (Word Level)\n",
    "en_tokenizer = Tokenizer(WordLevel(unk_token=unknown_token))\n",
    "en_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# 독일어 토크나이저 정의 (Word Level)\n",
    "de_tokenizer = Tokenizer(WordLevel(unk_token=unknown_token))\n",
    "de_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e647190d-0467-4f21-a78f-6c23be03f702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 trainer 설정 (vocab 생성)\n",
    "pad_token, sos_token, eos_token = \"<pad>\", \"<sos>\", \"<eos>\"\n",
    "special_tokens = [unknown_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=special_tokens, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8a70fe9f-edee-4693-b62e-ae26f6daad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 학습 (문장 리스트 기반)\n",
    "train_en = train_dataset['en']\n",
    "train_de = train_dataset['de']\n",
    "\n",
    "en_tokenizer.train_from_iterator(train_en, trainer=trainer)\n",
    "de_tokenizer.train_from_iterator(train_de, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "587e362b-bfd4-4c85-bd14-fe342ad8eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EN] vocab size: 6203\n",
      "[DE] vocab size: 8060\n",
      "[EN] Sample EN vocab tokens: ['shave', 'exterior', 'serious', 'medium', 'four', 'onlookers', 'waterfront', 'meal', 'called', 'auditorium']\n",
      "[DE] Sample DE vocab tokens: ['Schwimmweste', 'Jugendlicher', 'weicht', 'leisten', 'veranstaltet', 'Betonkonstruktion', 'Veranstaltung', 'Winterkleidung', 'verkehrsreiche', 'First']\n"
     ]
    }
   ],
   "source": [
    "# vocab 확인\n",
    "\n",
    "print(\"[EN] vocab size: {}\".format(en_tokenizer.get_vocab_size()))\n",
    "print(\"[DE] vocab size: {}\".format(de_tokenizer.get_vocab_size()))\n",
    "\n",
    "print(\"[EN] Sample EN vocab tokens: {}\".format(list(en_tokenizer.get_vocab().keys())[:10]))\n",
    "print(\"[DE] Sample DE vocab tokens: {}\".format(list(de_tokenizer.get_vocab().keys())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "98fafa2f-2e9f-4d3a-aaf5-c736c8a6085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EN] special_token: <unk>, index: 0\n",
      "[DE] special token: <unk>, index: 0\n",
      "[EN] special_token: <pad>, index: 1\n",
      "[DE] special token: <pad>, index: 1\n",
      "[EN] special_token: <sos>, index: 2\n",
      "[DE] special token: <sos>, index: 2\n",
      "[EN] special_token: <eos>, index: 3\n",
      "[DE] special token: <eos>, index: 3\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 인덱스 체크\n",
    "\n",
    "# <unk> : 0\n",
    "# <pad> : 1\n",
    "# <sos> : 2\n",
    "# <eos> : 3\n",
    "for special_token in special_tokens:\n",
    "    print(\"[EN] special_token: {}, index: {}\".format(special_token, en_tokenizer.get_vocab()[special_token]))\n",
    "    print(\"[DE] special token: {}, index: {}\".format(special_token, de_tokenizer.get_vocab()[special_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0d18e-2cef-474e-a572-b8bdc658fa1a",
   "metadata": {},
   "source": [
    "- 데이터 전처리\n",
    "  - 데이터 패딩 등.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "86833fa3-645c-4840-ac1a-73ce44df1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sos_id, en_eos_id = en_tokenizer.token_to_id(sos_token), en_tokenizer.token_to_id(eos_token)\n",
    "de_sos_id, de_eos_id = de_tokenizer.token_to_id(sos_token), de_tokenizer.token_to_id(eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eefa03-45fa-4c3c-8f60-57dfdeae8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data: dict) -> dict:\n",
    "    # 토큰 id로 변환\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
