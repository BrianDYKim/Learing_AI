{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cb1658-7f23-48b0-8338-09ab6f96c12d",
   "metadata": {},
   "source": [
    "## Self Attention with Relative Position Representations 논문 실습\n",
    "\n",
    "- 본 논문은 Attention is all you need (NIPS 2017) 에서 제안한 Transformer Architecture를 기반으로 실습합니다.\n",
    "- Attention is all you need 에서 제안한 아키텍처 상에서 Self-Attention 모듈만 개선함으로써 성능 개선을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558da2c-e4fc-4bd2-b3c2-998a4c715ba3",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 (PreProcessing)\n",
    "- 허깅페이스 API를 이용해서 대표적인 영어-독어 데이터셋인 **Multi30k** 를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056cbe37-95f0-4f19-9aaa-54ab543fb579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3b2c674-b6bd-4998-82cf-04c74a35c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Two young, White males are outside near many bushes.', 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160182e9-6982-49b5-b197-a934ca723607",
   "metadata": {},
   "source": [
    "- **Tokenizer** 및 **Vocab** 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cac26e2-e0bf-4109-b980-5b7e773a8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a52f0232-6691-4b20-99f7-0a9ee03644a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"<unk>\"\n",
    "\n",
    "def initialize_tokenizer() -> Tokenizer:\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=unknown_token))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer\n",
    "\n",
    "de_tokenizer, en_tokenizer = [initialize_tokenizer() for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e187fc4-8014-4a50-98b9-a577e3004c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 trainer 생성\n",
    "pad_token, sos_token, eos_token = \"<pad>\", \"<sos>\", \"<eos>\"\n",
    "special_tokens = [unknown_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=special_tokens, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d937490b-86bf-4b96-98e8-42f0afe26a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 학습\n",
    "train_de, train_en = train_dataset['de'], train_dataset['en']\n",
    "\n",
    "de_tokenizer.train_from_iterator(train_de, trainer=trainer)\n",
    "en_tokenizer.train_from_iterator(train_en, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "415b24b8-665e-46ef-9deb-c7d27ebe2732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DE] vocab size: 8060\n",
      "[EN] vocab size: 6203\n",
      "[DE] Sample DE vocab tokens: ['braunhaariger', 'Hochstart', 'gescheckter', 'Picknick', 'Ellenbogen', 'kürzlich', 'zuhört', 'anschneiden', 'plantscht', 'sucht']\n",
      "[EN] Sample EN vocab tokens: ['final', 'pastor', 'brown', 'crowded', 'classroom', 'practicing', 'peace', 'blazer', 'pancake', 'designing']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 학습 결과 확인\n",
    "\n",
    "print(\"[DE] vocab size: {}\".format(de_tokenizer.get_vocab_size()))\n",
    "print(\"[EN] vocab size: {}\".format(en_tokenizer.get_vocab_size()))\n",
    "\n",
    "print(\"[DE] Sample DE vocab tokens: {}\".format(list(de_tokenizer.get_vocab().keys())[:10]))\n",
    "print(\"[EN] Sample EN vocab tokens: {}\".format(list(en_tokenizer.get_vocab().keys())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1451ccd1-d094-475b-8022-6ba5cf2c7c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DE] special token: <unk>, index: 0\n",
      "[EN] special token: <unk>, index: 0\n",
      "[DE] special token: <pad>, index: 1\n",
      "[EN] special token: <pad>, index: 1\n",
      "[DE] special token: <sos>, index: 2\n",
      "[EN] special token: <sos>, index: 2\n",
      "[DE] special token: <eos>, index: 3\n",
      "[EN] special token: <eos>, index: 3\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 체크\n",
    "for special_token in special_tokens:\n",
    "    print(\"[DE] special token: {}, index: {}\".format(special_token, de_tokenizer.get_vocab()[special_token]))\n",
    "    print(\"[EN] special token: {}, index: {}\".format(special_token, en_tokenizer.get_vocab()[special_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae21a6-f75e-4a27-b497-dacd04e297f1",
   "metadata": {},
   "source": [
    "- 하이퍼 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffd7ef6a-9ff5-433d-b5e7-9bc34018f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfiguration:\n",
    "    def __init__(self, \n",
    "                 max_len: int = 768, \n",
    "                 batch_size: int = 32, \n",
    "                 hidden_size: int = 512, \n",
    "                 ffn_size: int = 2048,\n",
    "                 num_heads: int = 8, \n",
    "                 num_layers: int = 6, \n",
    "                 dropout_pb: float = 0.1, \n",
    "                 src_vocab_size: int = 0, \n",
    "                 trg_vocab_size: int = 0\n",
    "                ):\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_size = ffn_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_pb = dropout_pb\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "\n",
    "model_config = ModelConfiguration(src_vocab_size=de_tokenizer.get_vocab_size(), trg_vocab_size=en_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e634d2-9025-4b26-8964-7f67bd2ddb5b",
   "metadata": {},
   "source": [
    "- 데이터 전처리\n",
    "    - 데이터 패딩 등..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96eda891-078c-4e93-b98b-3f5e695049c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_pad_id, en_pad_id = de_tokenizer.token_to_id(pad_token), en_tokenizer.token_to_id(pad_token)\n",
    "de_sos_id, en_sos_id = de_tokenizer.token_to_id(sos_token), en_tokenizer.token_to_id(sos_token)\n",
    "de_eos_id, en_eos_id = de_tokenizer.token_to_id(eos_token), en_tokenizer.token_to_id(eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50c98003-8131-43a8-b2ad-b8de9727a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: {\"en\" : \"example_en\", \"de\" : \"example_de\"}\n",
    "# output: {\"encoder_input_ids\": [], \"encoder_attention_mask\": [], \"decoder_input_ids\": [], \"decoder_attention_mask\": [], \"labels\": []}\n",
    "def preprocess(dataset: dict) -> dict:\n",
    "    max_len = model_config.max_len\n",
    "\n",
    "    # 토큰 id로 변환\n",
    "    src_input_ids = de_tokenizer.encode(dataset['de']).ids\n",
    "    trg_input_ids = en_tokenizer.encode(dataset['en']).ids\n",
    "\n",
    "    # decoder input\n",
    "    decoder_input = [en_sos_id] + trg_input_ids\n",
    "    labels = trg_input_ids + [en_eos_id]\n",
    "\n",
    "    # padding\n",
    "    encoder_input = src_input_ids[:max_len] + [de_pad_id] * max(0, max_len - len(src_input_ids))\n",
    "    decoder_input = decoder_input[:max_len] + [en_pad_id] * max(0, max_len - len(decoder_input))\n",
    "    labels = labels[:max_len] + [en_pad_id] * max(0, max_len - len(labels))\n",
    "    # Optional. loss 계산시 pad_id를 계산하지 않도록 ignore_index 적용\n",
    "    labels = [token if token != en_pad_id else -100 for token in labels]\n",
    "\n",
    "    # Attention mask\n",
    "    encoder_attention_mask = [1 if token != de_pad_id else 0 for token in encoder_input]\n",
    "    decoder_attention_mask = [1 if token != en_pad_id else 0 for token in decoder_input]\n",
    "\n",
    "    return {\n",
    "        \"encoder_input_ids\" : encoder_input,\n",
    "        \"encoder_attention_mask\" : encoder_attention_mask,\n",
    "        \"decoder_input_ids\" : decoder_input,\n",
    "        \"decoder_attention_mask\" : decoder_attention_mask,\n",
    "        \"labels\" : labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03326ffc-d8c1-4c8b-b41a-8e9dda0c0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████| 29000/29000 [00:08<00:00, 3587.86 examples/s]\n",
      "Map: 100%|█████████████████████████| 1014/1014 [00:00<00:00, 3741.02 examples/s]\n",
      "Map: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 3695.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess, remove_columns=['en', 'de'])\n",
    "validation_dataset = validation_dataset.map(preprocess, remove_columns=['en', 'de'])\n",
    "test_dataset = test_dataset.map(preprocess, remove_columns=['en', 'de'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15239652-b653-4410-8e80-5b362ff0356b",
   "metadata": {},
   "source": [
    "- DataLoader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c8f11139-4af4-44d9-b5bb-488f3429022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_function(batch):\n",
    "    return {\n",
    "        key: torch.tensor([data[key] for data in batch], dtype=torch.long) for key in batch[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b77e2487-67c0-4049-bd51-3cf0dd39854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = model_config.batch_size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_function)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_function)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d4addc04-ad63-45c5-ae31-2ace950ae13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_ids: shape=torch.Size([32, 768])\n",
      "encoder_attention_mask: shape=torch.Size([32, 768])\n",
      "decoder_input_ids: shape=torch.Size([32, 768])\n",
      "decoder_attention_mask: shape=torch.Size([32, 768])\n",
      "labels: shape=torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "# 배치 샘플 확인\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "for key, value in batch.items():\n",
    "    print(\"{}: shape={}\".format(key, value.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be156610-45e2-458a-ab09-97361860d9c0",
   "metadata": {},
   "source": [
    "#### 토큰 임베딩\n",
    "- 해당 실습에서는 파동 함수가 아닌 학습 임베딩을 이용하여 실습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1a33fae7-29bd-44de-8444-7485c6f10b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 학습 device 정의\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "eps = 1e-12\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "13790fd1-5776-475d-bb1b-9a67147d7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_len: int, dropout_pb: float):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.positional_embedding = nn.Embedding(max_len, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps=eps)\n",
    "        self.dropout = nn.Dropout(dropout_pb)\n",
    "\n",
    "    # input: (batch_size, max_len)\n",
    "    # output: (batch_size, max_len, hidden_size)\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # positional sequence 생성\n",
    "        sequence_len = input_ids.size(1) # max_len\n",
    "        positional_ids = torch.arange(sequence_len, device=device).unsqueeze(0).expand_as(input_ids) # (batch_size, max_len)\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.token_embedding(input_ids) # (batch_size, max_len, hidden_size)\n",
    "        positional_embeddings = self.positional_embedding(positional_ids) # (batch_size, max_len, hidden_size)\n",
    "\n",
    "        # Add/Norm -> Dropout\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121cee0-1642-45f4-8872-247fa06b7a14",
   "metadata": {},
   "source": [
    "- 임베딩 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "53eea5e4-61ee-4817-89d0-3129f1a2e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([32, 768])\n",
      "Embedding Shape: torch.Size([32, 768, 512])\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 검증\n",
    "embedding_layer = Embeddings(\n",
    "    vocab_size=model_config.src_vocab_size,\n",
    "    hidden_size=model_config.hidden_size,\n",
    "    max_len=model_config.max_len,\n",
    "    dropout_pb=model_config.dropout_pb\n",
    ").to(device)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "input_ids = batch['encoder_input_ids'].to(device)\n",
    "\n",
    "embeddings = embedding_layer(input_ids)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Input Shape: {}\".format(input_ids.shape))\n",
    "print(\"Embedding Shape: {}\".format(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcbac2-cdaa-4aa8-9825-1574dd2ff0a7",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention 구현\n",
    "\n",
    "- Transformer 아키텍처의 핵심인 멀티 헤드 어텐션을 구현합니다.\n",
    "    - **scaled-dot-product attention** 구현\n",
    "    - **Attention Head** 구현\n",
    "    - Attention Head를 조합하여 **Multi-Head Attention** 구현\n",
    "- 이 때, 해당 논문에서 제시한 상대 위치 임베딩을 추가적으로 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "92a5e12a-6e3b-450f-bb83-1b1ac7d91ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# query: (batch_size, max_len, d_head)\n",
    "# key: (batch_size, max_len, d_head)\n",
    "# value: (batch_size, max_len, d_head)\n",
    "# output: (batch_size, max_len, d_head)\n",
    "def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "    # MatMul (max_len, d_head) * (d_head, max_len) = (batch_size, max_len, max_len)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2))\n",
    "\n",
    "    # Scale by sqrt(d_head)\n",
    "    dim_k = query.size(-1)\n",
    "    scores = scores / (dim_k ** 0.5)\n",
    "\n",
    "    # Masking if mask is valid\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # Softmax (batch_size, max_len, max_len)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # MatMul (max_len, max_len) * (max_len, d_head) = (batch_size, max_len, d_head)\n",
    "    output = torch.bmm(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea7a03-5410-4a85-bf67-ee6c5e4b5f04",
   "metadata": {},
   "source": [
    "- **Attention Head 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f56d45a1-fe71-475a-bbaa-2a01d6c534b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, head_dim: int):\n",
    "        super().__init__()\n",
    "        self.query_projection = nn.Linear(hidden_dim, head_dim)\n",
    "        self.key_projection = nn.Linear(hidden_dim, head_dim)\n",
    "        self.value_projection = nn.Linear(hidden_dim, head_dim)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        Q = self.query_projection(query)\n",
    "        K = self.key_projection(key)\n",
    "        V = self.value_projection(value)\n",
    "\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8710ee-89a0-4194-85ca-37f1d7d0d0da",
   "metadata": {},
   "source": [
    "- **Multi-Head Attention** 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2e74ad6d-d73f-4904-b945-c82ed60e71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.head_list = [AttentionHead(hidden_dim, head_dim) for _ in range(num_heads)]\n",
    "        self.output_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        concatenated_attention = torch.concat([head(query, key, value, mask) for head in self.head_list], dim=-1)\n",
    "        output = self.output_linear(concatenated_attention)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61814d56-6607-4a54-9896-fd46d6f7f720",
   "metadata": {},
   "source": [
    "- 멀티 헤드 어텐션 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a16e3a9f-c435-4603-90bd-0abfc50030da",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor for argument weight is on cpu but expected on mps",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m multi_head_attention \u001b[38;5;241m=\u001b[39m MultiHeadAttention(hidden_dim \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mhidden_size, num_heads \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mnum_heads)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_head_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(attn_output\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/playground/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/playground/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[142], line 10\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: torch\u001b[38;5;241m.\u001b[39mTensor, key: torch\u001b[38;5;241m.\u001b[39mTensor, value: torch\u001b[38;5;241m.\u001b[39mTensor, mask: torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 10\u001b[0m     concatenated_attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([head(query, key, value, mask) \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_list], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_linear(concatenated_attention)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[142], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: torch\u001b[38;5;241m.\u001b[39mTensor, key: torch\u001b[38;5;241m.\u001b[39mTensor, value: torch\u001b[38;5;241m.\u001b[39mTensor, mask: torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 10\u001b[0m     concatenated_attention \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_list], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_linear(concatenated_attention)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/playground/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/playground/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[119], line 9\u001b[0m, in \u001b[0;36mAttentionHead.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: torch\u001b[38;5;241m.\u001b[39mTensor, key: torch\u001b[38;5;241m.\u001b[39mTensor, value: torch\u001b[38;5;241m.\u001b[39mTensor, mask: torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m----> 9\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_projection(key)\n\u001b[1;32m     11\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_projection(value)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/playground/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/playground/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/playground/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor for argument weight is on cpu but expected on mps"
     ]
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(hidden_dim = model_config.hidden_size, num_heads = model_config.num_heads).to(device)\n",
    "\n",
    "attn_output = multi_head_attention(embeddings, embeddings, embeddings)\n",
    "\n",
    "print(attn_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2d65e-6f9f-43fc-8687-13f79ac93f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
