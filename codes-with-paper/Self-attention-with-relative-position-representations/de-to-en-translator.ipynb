{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cb1658-7f23-48b0-8338-09ab6f96c12d",
   "metadata": {},
   "source": [
    "## Self Attention with Relative Position Representations 논문 실습\n",
    "\n",
    "- 본 논문은 Attention is all you need (NIPS 2017) 에서 제안한 Transformer Architecture를 기반으로 실습합니다.\n",
    "- Attention is all you need 에서 제안한 아키텍처 상에서 Self-Attention 모듈만 개선함으로써 성능 개선을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558da2c-e4fc-4bd2-b3c2-998a4c715ba3",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 (PreProcessing)\n",
    "- 허깅페이스 API를 이용해서 대표적인 영어-독어 데이터셋인 **Multi30k** 를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056cbe37-95f0-4f19-9aaa-54ab543fb579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/playground/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b2c674-b6bd-4998-82cf-04c74a35c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Two young, White males are outside near many bushes.', 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160182e9-6982-49b5-b197-a934ca723607",
   "metadata": {},
   "source": [
    "- **Tokenizer** 및 **Vocab** 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cac26e2-e0bf-4109-b980-5b7e773a8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52f0232-6691-4b20-99f7-0a9ee03644a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"<unk>\"\n",
    "\n",
    "def initialize_tokenizer() -> Tokenizer:\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=unknown_token))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer\n",
    "\n",
    "de_tokenizer, en_tokenizer = [initialize_tokenizer() for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e187fc4-8014-4a50-98b9-a577e3004c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 trainer 생성\n",
    "pad_token, sos_token, eos_token = \"<pad>\", \"<sos>\", \"<eos>\"\n",
    "special_tokens = [unknown_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=special_tokens, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d937490b-86bf-4b96-98e8-42f0afe26a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 학습\n",
    "train_de, train_en = train_dataset['de'], train_dataset['en']\n",
    "\n",
    "de_tokenizer.train_from_iterator(train_de, trainer=trainer)\n",
    "en_tokenizer.train_from_iterator(train_en, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "415b24b8-665e-46ef-9deb-c7d27ebe2732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DE] vocab size: 8060\n",
      "[EN] vocab size: 6203\n",
      "[DE] Sample DE vocab tokens: ['Spielkonsole', 'zuläuft', 'Barkeeperin', 'kontrolliert', 'rettet', 'Mädchengruppe', 'Esel', 'Hochzeit', 'verloren', 'oder']\n",
      "[EN] Sample EN vocab tokens: ['juice', 'pajama', 'watched', 'Hopper', 'architectural', 'onto', 'engaging', 'tortillas', 'away', 'littered']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 학습 결과 확인\n",
    "\n",
    "print(\"[DE] vocab size: {}\".format(de_tokenizer.get_vocab_size()))\n",
    "print(\"[EN] vocab size: {}\".format(en_tokenizer.get_vocab_size()))\n",
    "\n",
    "print(\"[DE] Sample DE vocab tokens: {}\".format(list(de_tokenizer.get_vocab().keys())[:10]))\n",
    "print(\"[EN] Sample EN vocab tokens: {}\".format(list(en_tokenizer.get_vocab().keys())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1451ccd1-d094-475b-8022-6ba5cf2c7c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DE] special token: <unk>, index: 0\n",
      "[EN] special token: <unk>, index: 0\n",
      "[DE] special token: <pad>, index: 1\n",
      "[EN] special token: <pad>, index: 1\n",
      "[DE] special token: <sos>, index: 2\n",
      "[EN] special token: <sos>, index: 2\n",
      "[DE] special token: <eos>, index: 3\n",
      "[EN] special token: <eos>, index: 3\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 체크\n",
    "for special_token in special_tokens:\n",
    "    print(\"[DE] special token: {}, index: {}\".format(special_token, de_tokenizer.get_vocab()[special_token]))\n",
    "    print(\"[EN] special token: {}, index: {}\".format(special_token, en_tokenizer.get_vocab()[special_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae21a6-f75e-4a27-b497-dacd04e297f1",
   "metadata": {},
   "source": [
    "- 하이퍼 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ffd7ef6a-9ff5-433d-b5e7-9bc34018f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfiguration:\n",
    "    def __init__(self, \n",
    "                 max_len: int = 768, \n",
    "                 batch_size: int = 32, \n",
    "                 hidden_size: int = 512, \n",
    "                 ffn_size: int = 2048,\n",
    "                 num_heads: int = 8, \n",
    "                 num_layers: int = 6, \n",
    "                 dropout_pb: float = 0.1, \n",
    "                 eps: float = 1e-12, \n",
    "                 max_relative_position: int = 128, \n",
    "                 src_vocab_size: int = 0, \n",
    "                 trg_vocab_size: int = 0\n",
    "                ):\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_size = ffn_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_pb = dropout_pb\n",
    "        self.eps = eps\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "\n",
    "model_config = ModelConfiguration(src_vocab_size=de_tokenizer.get_vocab_size(), trg_vocab_size=en_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e634d2-9025-4b26-8964-7f67bd2ddb5b",
   "metadata": {},
   "source": [
    "- 데이터 전처리\n",
    "    - 데이터 패딩 등..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96eda891-078c-4e93-b98b-3f5e695049c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_pad_id, en_pad_id = de_tokenizer.token_to_id(pad_token), en_tokenizer.token_to_id(pad_token)\n",
    "de_sos_id, en_sos_id = de_tokenizer.token_to_id(sos_token), en_tokenizer.token_to_id(sos_token)\n",
    "de_eos_id, en_eos_id = de_tokenizer.token_to_id(eos_token), en_tokenizer.token_to_id(eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c98003-8131-43a8-b2ad-b8de9727a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: {\"en\" : \"example_en\", \"de\" : \"example_de\"}\n",
    "# output: {\"encoder_input_ids\": [], \"encoder_attention_mask\": [], \"decoder_input_ids\": [], \"decoder_attention_mask\": [], \"labels\": []}\n",
    "def preprocess(dataset: dict) -> dict:\n",
    "    max_len = model_config.max_len\n",
    "\n",
    "    # 토큰 id로 변환\n",
    "    src_input_ids = de_tokenizer.encode(dataset['de']).ids\n",
    "    trg_input_ids = en_tokenizer.encode(dataset['en']).ids\n",
    "\n",
    "    # decoder input\n",
    "    decoder_input = [en_sos_id] + trg_input_ids\n",
    "    labels = trg_input_ids + [en_eos_id]\n",
    "\n",
    "    # padding\n",
    "    encoder_input = src_input_ids[:max_len] + [de_pad_id] * max(0, max_len - len(src_input_ids))\n",
    "    decoder_input = decoder_input[:max_len] + [en_pad_id] * max(0, max_len - len(decoder_input))\n",
    "    labels = labels[:max_len] + [en_pad_id] * max(0, max_len - len(labels))\n",
    "    # Optional. loss 계산시 pad_id를 계산하지 않도록 ignore_index 적용\n",
    "    labels = [token if token != en_pad_id else -100 for token in labels]\n",
    "\n",
    "    # Attention mask\n",
    "    encoder_attention_mask = [1 if token != de_pad_id else 0 for token in encoder_input]\n",
    "    decoder_attention_mask = [1 if token != en_pad_id else 0 for token in decoder_input]\n",
    "\n",
    "    return {\n",
    "        \"encoder_input_ids\" : encoder_input,\n",
    "        \"encoder_attention_mask\" : encoder_attention_mask,\n",
    "        \"decoder_input_ids\" : decoder_input,\n",
    "        \"decoder_attention_mask\" : decoder_attention_mask,\n",
    "        \"labels\" : labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03326ffc-d8c1-4c8b-b41a-8e9dda0c0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████| 29000/29000 [00:08<00:00, 3621.94 examples/s]\n",
      "Map: 100%|█████████████████████████| 1014/1014 [00:00<00:00, 3628.54 examples/s]\n",
      "Map: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 3574.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess, remove_columns=['en', 'de'])\n",
    "validation_dataset = validation_dataset.map(preprocess, remove_columns=['en', 'de'])\n",
    "test_dataset = test_dataset.map(preprocess, remove_columns=['en', 'de'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15239652-b653-4410-8e80-5b362ff0356b",
   "metadata": {},
   "source": [
    "- DataLoader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8f11139-4af4-44d9-b5bb-488f3429022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_function(batch):\n",
    "    return {\n",
    "        key: torch.tensor([data[key] for data in batch], dtype=torch.long) for key in batch[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b77e2487-67c0-4049-bd51-3cf0dd39854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = model_config.batch_size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_function)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_function)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4addc04-ad63-45c5-ae31-2ace950ae13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_ids: shape=torch.Size([32, 768])\n",
      "encoder_attention_mask: shape=torch.Size([32, 768])\n",
      "decoder_input_ids: shape=torch.Size([32, 768])\n",
      "decoder_attention_mask: shape=torch.Size([32, 768])\n",
      "labels: shape=torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "# 배치 샘플 확인\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "for key, value in batch.items():\n",
    "    print(\"{}: shape={}\".format(key, value.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be156610-45e2-458a-ab09-97361860d9c0",
   "metadata": {},
   "source": [
    "#### 토큰 임베딩\n",
    "- 해당 실습에서는 파동 함수가 아닌 학습 임베딩을 이용하여 실습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a33fae7-29bd-44de-8444-7485c6f10b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 학습 device 정의\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13790fd1-5776-475d-bb1b-9a67147d7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_len: int, dropout_pb: float, eps: float):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.positional_embedding = nn.Embedding(max_len, hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size, eps=eps)\n",
    "        self.dropout = nn.Dropout(dropout_pb)\n",
    "\n",
    "    # input: (batch_size, max_len)\n",
    "    # output: (batch_size, max_len, hidden_size)\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # positional sequence 생성\n",
    "        sequence_len = input_ids.size(1) # max_len\n",
    "        positional_ids = torch.arange(sequence_len, device=device).unsqueeze(0).expand_as(input_ids) # (batch_size, max_len)\n",
    "\n",
    "        # Embedding\n",
    "        token_embeddings = self.token_embedding(input_ids) # (batch_size, max_len, hidden_size)\n",
    "        positional_embeddings = self.positional_embedding(positional_ids) # (batch_size, max_len, hidden_size)\n",
    "\n",
    "        # Add/Norm -> Dropout\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121cee0-1642-45f4-8872-247fa06b7a14",
   "metadata": {},
   "source": [
    "- 임베딩 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53eea5e4-61ee-4817-89d0-3129f1a2e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([32, 768])\n",
      "Embedding Shape: torch.Size([32, 768, 512])\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 검증\n",
    "embedding_layer = Embeddings(\n",
    "    vocab_size=model_config.src_vocab_size,\n",
    "    hidden_size=model_config.hidden_size,\n",
    "    max_len=model_config.max_len,\n",
    "    dropout_pb=model_config.dropout_pb,\n",
    "    eps=model_config.eps\n",
    ").to(device)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "input_ids = batch['encoder_input_ids'].to(device)\n",
    "\n",
    "embeddings = embedding_layer(input_ids)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Input Shape: {}\".format(input_ids.shape))\n",
    "print(\"Embedding Shape: {}\".format(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcbac2-cdaa-4aa8-9825-1574dd2ff0a7",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention 구현\n",
    "\n",
    "- Transformer 아키텍처의 핵심인 멀티 헤드 어텐션을 구현합니다.\n",
    "    - **scaled-dot-product attention** 구현\n",
    "    - **Attention Head** 구현\n",
    "    - Attention Head를 조합하여 **Multi-Head Attention** 구현\n",
    "- **Scaled-dot product Attention with RPR (Relative Positional Representatives)** 을 이용하여 상대 위치 임베딩을 적용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "29be9779-5671-4228-83cd-3d6d5952ab01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 4, 5, 6],\n",
      "        [2, 3, 4, 5],\n",
      "        [1, 2, 3, 4],\n",
      "        [0, 1, 2, 3]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# playground\n",
    "seq_len = 4\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=device)\n",
    "# (seq_len, seq_len)\n",
    "relative_positions = position_ids[None, :] - position_ids[:, None]\n",
    "\n",
    "# 양수화 -> 2 * max_len - 1 개의 원소 경우의 수를 가짐\n",
    "relative_positions.clamp()\n",
    "\n",
    "print(relative_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30bd4d-f3ee-432c-9004-ba6cf65eff54",
   "metadata": {},
   "source": [
    "- **Attention Head 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f56d45a1-fe71-475a-bbaa-2a01d6c534b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, head_dim: int):\n",
    "        super().__init__()\n",
    "        self.query_projection = nn.Linear(hidden_dim, head_dim)\n",
    "        self.key_projection = nn.Linear(hidden_dim, head_dim)\n",
    "        self.value_projection = nn.Linear(hidden_dim, head_dim)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        Q = self.query_projection(query)\n",
    "        K = self.key_projection(key)\n",
    "        V = self.value_projection(value)\n",
    "\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    # query: (batch_size, max_len, d_head)\n",
    "    # key: (batch_size, max_len, d_head)\n",
    "    # value: (batch_size, max_len, d_head)\n",
    "    # output: Attention_weight(batch_size, max_len, d_head), Output(batch_size, max_len, d_head)\n",
    "    def scaled_dot_product_attention(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: torch.Tensor=None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # hidden size\n",
    "        dim_k = query.size(-1)\n",
    "    \n",
    "        # (batch_size, max_len, d_head) * (batch_size, d_head, max_len) = (batch_size, max_len, max_len)\n",
    "        # scaling by sqrt(dim_k)\n",
    "        scores = torch.bmm(query, key.transpose(1, 2)) / (dim_k ** 0.5)\n",
    "    \n",
    "        # mask가 존재하면 -1e9를 더한다\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # (batch_size, max_len, max_len) -> (batch_size, max_len, max_len) softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # (batch_size, max_len, max_len) * (batch_size, max_len, d_head) = (batch_size, max_len, d_head)\n",
    "        output = torch.bmm(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def scaled_dot_product_attention_with_rpr(\n",
    "        self, \n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        relative_embeddings: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8710ee-89a0-4194-85ca-37f1d7d0d0da",
   "metadata": {},
   "source": [
    "- **Multi-Head Attention** 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2e74ad6d-d73f-4904-b945-c82ed60e71db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.head_list = nn.ModuleList([AttentionHead(hidden_dim, head_dim) for _ in range(num_heads)])\n",
    "        self.output_linear = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n",
    "        concatenated_attention = torch.concat([head(query, key, value, mask) for head in self.head_list], dim=-1)\n",
    "        output = self.output_linear(concatenated_attention)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61814d56-6607-4a54-9896-fd46d6f7f720",
   "metadata": {},
   "source": [
    "- 멀티 헤드 어텐션 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a16e3a9f-c435-4603-90bd-0abfc50030da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768, 512])\n"
     ]
    }
   ],
   "source": [
    "multi_head_attention = MultiHeadAttention(hidden_dim = model_config.hidden_size, num_heads = model_config.num_heads).to(device)\n",
    "\n",
    "attn_output = multi_head_attention(embeddings, embeddings, embeddings)\n",
    "\n",
    "print(attn_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2d65e-6f9f-43fc-8687-13f79ac93f60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
