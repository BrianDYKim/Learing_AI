{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cb1658-7f23-48b0-8338-09ab6f96c12d",
   "metadata": {},
   "source": [
    "## Self Attention with Relative Position Representations 논문 실습\n",
    "\n",
    "- 본 논문은 Attention is all you need (NIPS 2017) 에서 제안한 Transformer Architecture를 기반으로 실습합니다.\n",
    "- Attention is all you need 에서 제안한 아키텍처 상에서 Self-Attention 모듈만 개선함으로써 성능 개선을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558da2c-e4fc-4bd2-b3c2-998a4c715ba3",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 (PreProcessing)\n",
    "- 허깅페이스 API를 이용해서 대표적인 영어-독어 데이터셋인 **Multi30k** 를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "056cbe37-95f0-4f19-9aaa-54ab543fb579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = dataset['train'], dataset['validation'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3b2c674-b6bd-4998-82cf-04c74a35c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 'Two young, White males are outside near many bushes.', 'de': 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160182e9-6982-49b5-b197-a934ca723607",
   "metadata": {},
   "source": [
    "- **Tokenizer** 및 **Vocab** 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cac26e2-e0bf-4109-b980-5b7e773a8991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a52f0232-6691-4b20-99f7-0a9ee03644a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_token = \"<unk>\"\n",
    "\n",
    "def initialize_tokenizer() -> Tokenizer:\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=unknown_token))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer\n",
    "\n",
    "de_tokenizer, en_tokenizer = [initialize_tokenizer() for _ in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e187fc4-8014-4a50-98b9-a577e3004c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 trainer 생성\n",
    "pad_token, sos_token, eos_token = \"<pad>\", \"<sos>\", \"<eos>\"\n",
    "special_tokens = [unknown_token, pad_token, sos_token, eos_token]\n",
    "\n",
    "trainer = WordLevelTrainer(special_tokens=special_tokens, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d937490b-86bf-4b96-98e8-42f0afe26a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 학습\n",
    "train_de, train_en = train_dataset['de'], train_dataset['en']\n",
    "\n",
    "de_tokenizer.train_from_iterator(train_de, trainer=trainer)\n",
    "en_tokenizer.train_from_iterator(train_en, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "415b24b8-665e-46ef-9deb-c7d27ebe2732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DE] vocab size: 8060\n",
      "[EN] vocab size: 6203\n",
      "[DE] Sample DE vocab tokens: ['braunhaariger', 'Hochstart', 'gescheckter', 'Picknick', 'Ellenbogen', 'kürzlich', 'zuhört', 'anschneiden', 'plantscht', 'sucht']\n",
      "[EN] Sample EN vocab tokens: ['final', 'pastor', 'brown', 'crowded', 'classroom', 'practicing', 'peace', 'blazer', 'pancake', 'designing']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer 학습 결과 확인\n",
    "\n",
    "print(\"[DE] vocab size: {}\".format(de_tokenizer.get_vocab_size()))\n",
    "print(\"[EN] vocab size: {}\".format(en_tokenizer.get_vocab_size()))\n",
    "\n",
    "print(\"[DE] Sample DE vocab tokens: {}\".format(list(de_tokenizer.get_vocab().keys())[:10]))\n",
    "print(\"[EN] Sample EN vocab tokens: {}\".format(list(en_tokenizer.get_vocab().keys())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1451ccd1-d094-475b-8022-6ba5cf2c7c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DE] special token: <unk>, index: 0\n",
      "[EN] special token: <unk>, index: 0\n",
      "[DE] special token: <pad>, index: 1\n",
      "[EN] special token: <pad>, index: 1\n",
      "[DE] special token: <sos>, index: 2\n",
      "[EN] special token: <sos>, index: 2\n",
      "[DE] special token: <eos>, index: 3\n",
      "[EN] special token: <eos>, index: 3\n"
     ]
    }
   ],
   "source": [
    "# 특수 토큰 체크\n",
    "for special_token in special_tokens:\n",
    "    print(\"[DE] special token: {}, index: {}\".format(special_token, de_tokenizer.get_vocab()[special_token]))\n",
    "    print(\"[EN] special token: {}, index: {}\".format(special_token, en_tokenizer.get_vocab()[special_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae21a6-f75e-4a27-b497-dacd04e297f1",
   "metadata": {},
   "source": [
    "- 하이퍼 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffd7ef6a-9ff5-433d-b5e7-9bc34018f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfiguration:\n",
    "    def __init__(self, \n",
    "                 max_len: int = 768, \n",
    "                 batch_size: int = 32, \n",
    "                 hidden_size: int = 512, \n",
    "                 ffn_size: int = 2048,\n",
    "                 num_heads: int = 8, \n",
    "                 num_layers: int = 6, \n",
    "                 dropout_pb: float = 0.1, \n",
    "                 src_vocab_size: int = 0, \n",
    "                 trg_vocab_size: int = 0\n",
    "                ):\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.ffn_size = ffn_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_pb = dropout_pb\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "\n",
    "model_config = ModelConfiguration(src_vocab_size=de_tokenizer.get_vocab_size(), trg_vocab_size=en_tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e634d2-9025-4b26-8964-7f67bd2ddb5b",
   "metadata": {},
   "source": [
    "- 데이터 전처리\n",
    "    - 데이터 패딩 등..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96eda891-078c-4e93-b98b-3f5e695049c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_pad_id, en_pad_id = de_tokenizer.token_to_id(pad_token), en_tokenizer.token_to_id(pad_token)\n",
    "de_sos_id, en_sos_id = de_tokenizer.token_to_id(sos_token), en_tokenizer.token_to_id(sos_token)\n",
    "de_eos_id, en_eos_id = de_tokenizer.token_to_id(eos_token), en_tokenizer.token_to_id(eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50c98003-8131-43a8-b2ad-b8de9727a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: {\"en\" : \"example_en\", \"de\" : \"example_de\"}\n",
    "# output: {\"encoder_input_ids\": [], \"encoder_attention_mask\": [], \"decoder_input_ids\": [], \"decoder_attention_mask\": [], \"labels\": []}\n",
    "def preprocess(dataset: dict) -> dict:\n",
    "    max_len = model_config.max_len\n",
    "\n",
    "    # 토큰 id로 변환\n",
    "    src_input_ids = de_tokenizer.encode(dataset['de']).ids\n",
    "    trg_input_ids = en_tokenizer.encode(dataset['en']).ids\n",
    "\n",
    "    # decoder input\n",
    "    decoder_input = [en_sos_id] + trg_input_ids\n",
    "    labels = trg_input_ids + [en_eos_id]\n",
    "\n",
    "    # padding\n",
    "    encoder_input = src_input_ids[:max_len] + [de_pad_id] * max(0, max_len - len(src_input_ids))\n",
    "    decoder_input = decoder_input[:max_len] + [en_pad_id] * max(0, max_len - len(decoder_input))\n",
    "    labels = labels[:max_len] + [en_pad_id] * max(0, max_len - len(labels))\n",
    "    # Optional. loss 계산시 pad_id를 계산하지 않도록 ignore_index 적용\n",
    "    labels = [token if token != en_pad_id else -100 for token in labels]\n",
    "\n",
    "    # Attention mask\n",
    "    encoder_attention_mask = [1 if token != de_pad_id else 0 for token in encoder_input]\n",
    "    decoder_attention_mask = [1 if token != en_pad_id else 0 for token in decoder_input]\n",
    "\n",
    "    return {\n",
    "        \"encoder_input_ids\" : encoder_input,\n",
    "        \"encoder_attention_mask\" : encoder_attention_mask,\n",
    "        \"decoder_input_ids\" : decoder_input,\n",
    "        \"decoder_attention_mask\" : decoder_attention_mask,\n",
    "        \"labels\" : labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03326ffc-d8c1-4c8b-b41a-8e9dda0c0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████| 29000/29000 [00:08<00:00, 3587.86 examples/s]\n",
      "Map: 100%|█████████████████████████| 1014/1014 [00:00<00:00, 3741.02 examples/s]\n",
      "Map: 100%|█████████████████████████| 1000/1000 [00:00<00:00, 3695.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess, remove_columns=['en', 'de'])\n",
    "validation_dataset = validation_dataset.map(preprocess, remove_columns=['en', 'de'])\n",
    "test_dataset = test_dataset.map(preprocess, remove_columns=['en', 'de'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15239652-b653-4410-8e80-5b362ff0356b",
   "metadata": {},
   "source": [
    "- DataLoader 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c8f11139-4af4-44d9-b5bb-488f3429022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_function(batch):\n",
    "    return {\n",
    "        key: torch.tensor([data[key] for data in batch], dtype=torch.long) for key in batch[0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b77e2487-67c0-4049-bd51-3cf0dd39854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = model_config.batch_size\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_function)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_function)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4addc04-ad63-45c5-ae31-2ace950ae13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 샘플 확인\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "for key, value in batch.items():\n",
    "    print(\"{}: shape=\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
