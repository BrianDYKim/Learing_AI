{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe81e48-5fe7-40f9-b224-83d152ae4906",
   "metadata": {},
   "source": [
    "## 스케일드 점곱 어텐션\n",
    "\n",
    "### 스케일드 점곱 계산 절차\n",
    "- X = Query * Key (N x N 행렬 생성)\n",
    "- X = softmax(X / sqrt(d(model) / N(head)))\n",
    "- X_output = X * Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a5fb1503-c1cf-441e-abd1-b79fbc624d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2979e1ce-1c49-4ad0-9519-4cc2e2c643ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6c4907ed-cb7d-494b-bf48-3d3fb7550e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'time flies like an arrow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6b312774-5f56-4dbd-9272-df3499f0e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5dae9461-9a7d-4a57-ba4d-23c33b22be8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e9dcae66-9a92-4e5a-ae0b-d23862e807b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 임베딩\n",
    "config = AutoConfig.from_pretrained(model_ckpt)\n",
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "a0711395-0e0a-4dd1-9522-bfdf414b5571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embs = token_emb(inputs.input_ids)\n",
    "input_embs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "55cc4de7-9a0b-4c0d-822b-729bddd09670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "\n",
    "query = key = value = input_embs\n",
    "dim_k = key.size(-1)\n",
    "\n",
    "# 배치 행렬곱을 통해 어텐션 점수 출력 (hidden_dim 의 제곱근만큼 크기로 정규화 진행)\n",
    "scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "1a72a9d5-bd5c-4d5c-b365-4834ccb1a629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores에 softmax 적용\n",
    "import torch.nn.functional as F\n",
    "\n",
    "weights = F.softmax(scores, dim = -1)\n",
    "weights.sum(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8d647508-3ca6-4e7f-bb11-252ab4f71854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Key와 행렬곱 하여 점곱 어텐션 출력값을 생성\n",
    "attn_outputs = torch.bmm(weights, value)\n",
    "attn_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf25303-1ea2-4fe9-82b7-cec48e7ac16f",
   "metadata": {},
   "source": [
    "## Scaled dot product Attention 과정을 하나의 함수로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5a4ff7d9-0e00-46ae-a640-cf867408122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "    hidden_dim_size = query.size(-1)\n",
    "\n",
    "    # (Q * K_T) / sqrt(hidden_dim_size)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(hidden_dim_size)\n",
    "    # Softmax\n",
    "    weights = F.softmax(scores, dim = -1)\n",
    "    # output = weights * value\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07041db5-1a7c-40f3-8fec-bbb6b6386bc9",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "- 이전 Scaled-dot Product Attention 계산 절차와 다르게, Query, Key, Value 각각을 선형변환한 채로 Attention에 넣는 것이 차이\n",
    "- 이러한 선형변환은 여러벌로 구성되는데, 각 벌마다 집중하는 문맥이 다르기 때문에, 예측의 풍부함을 부여하는 효과를 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "28f81f6f-0b94-4d69-bcd9-d8575bf47634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Head 구현\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, head_dim: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    # (batch_size, seq_len, head_dim) 사이즈의 텐서를 리턴\n",
    "    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        query_output, key_output, value_output = self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)\n",
    "        return scaled_dot_product_attention(query=query_output, key=key_output, value=value_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "24ef429b-d612-48f8-9daf-71d02a4d7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head Attention 구현\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim, num_heads = config.hidden_size, config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([attention_head(hidden_state) for attention_head in self.heads], dim = -1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3e6a2a33-9128-4197-a659-8e22c74eb694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head_attn = MultiHeadAttention(config)\n",
    "attn_output = multi_head_attn(input_embs)\n",
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f0736-d70c-44ac-8f4c-3e0fd85328dd",
   "metadata": {},
   "source": [
    "## FeedForward Layer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fe73ace2-91f0-485b-8d1f-ade070ff3362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피드포워드 층 구현\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    # (batch_size, seq_len, hidden_state) 크기 그대로 feed_forward 시켜서 반환\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "bf2793dc-1299-4da5-98a0-7988e9a630ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_outputs)\n",
    "ff_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f25688-db46-41d0-bc6c-d335832a2e7a",
   "metadata": {},
   "source": [
    "## Transformer Encoder Layer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4041b9e8-469d-4860-b0b1-9fd50a21ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 사전 층 정규화를적용하고 입력을 쿼리, 키, 값 으로 복사합니다\n",
    "        hidden_state = self.layer_norm_1(x)\n",
    "        # 어텐션에 Gradient Highway 적용\n",
    "        x = x + self.attention(hidden_state)\n",
    "        # Gradient Highway와 FeedForward 적용\n",
    "        hidden_state = self.feed_forward(self.layer_norm_2(x))\n",
    "        return x + hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "778b0847-6bcd-40d7-af18-81956b896102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트\n",
    "encoder_layer = TransformerEncoderLayer(config)\n",
    "input_embs.shape, encoder_layer(input_embs).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5c9f3-71a4-4534-a0d3-50d139c853aa",
   "metadata": {},
   "source": [
    "## Transformer Token Embedding Layer 구현\n",
    "- 토큰 임베딩과 위치 임베딩을 더하여 Transformer 임베딩 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "af956d55-3411-45cb-ad87-3c2e0884f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # 입력 시퀀스에 대해서 위치 ID를 만듭니다\n",
    "        seq_length = input_ids.size(1)\n",
    "        positional_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n",
    "        # 토큰 임베딩과 위치 임베딩을 만듭니다\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        positional_embeddings = self.positional_embeddings(positional_ids)\n",
    "        # 토큰 임베딩과 위치 임베딩을 합칩니다 (Add/Norm + Dropout)\n",
    "        embeddings = token_embeddings + positional_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8d7e6a01-ce08-472c-bb85-fd3b0e0ac3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embeddings(config)\n",
    "embedding_layer(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ff312-b0b9-429d-8313-41a9658c491f",
   "metadata": {},
   "source": [
    "## 트랜스포머 인코더 생성\n",
    "- 임베딩 계층과 인코더 스택을 결합하여 트랜스포머 인코더를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3669c456-48e0-4d2f-9a9d-069272897d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embeddings(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fee8f1f9-8b6a-4a07-81f6-32b74fe56e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 768])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(config)\n",
    "encoder(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ddee0-f7ed-40ab-8e90-5ed9f666eeb1",
   "metadata": {},
   "source": [
    "## 인코더 단독으로 사용하는 경우 분류 헤드 추가\n",
    "- BERT 모델과 같은 텍스트 분류 문제는 인코더에 분류 헤드를 붙이는 형식으로 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c458ca14-1a22-41c1-b463-842728077901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # [CLS] 토큰의 은닉 상태를 선택\n",
    "        x = self.encoder(x)[:, 0, :]\n",
    "        x = self.dropout(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "cdf60e4d-3058-4371-91ca-35ca86b3b1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_labels = 3\n",
    "encoder_classifier = TransformerForSequenceClassification(config)\n",
    "encoder_classifier(inputs.input_ids).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208fe1f-9efd-4f65-8673-74993e5182dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
